---
layout: post
title: Generating melodies with PyTorch
---

There are not many articles about music generation via deep learning techniques, so here I'm sharing some of the main takeaways that I learnt by developing a simple recurrent network to generate melodies.

Check the full code here: [https://github.com/BouncyButton/PyTorch-MelodyGeneration](https://github.com/BouncyButton/PyTorch-MelodyGeneration)

# Introduction

I attended in the Department of Information Engineering (DEI) located in the University of Padua a [course](https://didattica.unipd.it/off/2019/LM/IN/IN2371/005PD/INP9086459/N0) about *Neural networks and deep learning*, taught by [Prof. Alberto Testolin](https://didattica.unipd.it/off/2019/LM/IN/IN2371/005PD/INP9086459/N0) and [Dr. Federico Chiarotti](https://www.dei.unipd.it/persona/EB99502C0B5BC54A2F2AFF873027AD4E).

This course consisted in experimenting with various topics that I will probably cover in other blog posts.

The project that I put mostly more effort considered the treatment of *sequences of data*: without getting too abstract, we were interested in models (i.e. black boxes) that can predict the next element in a sequence. We are in touch with similar technologies every day: for example, whenever we type some text in our smartphone keyboard, we receive suggestions about the next word that should be typed.

Similar mechanisms could be applied for the treatment of **music**: instead of predicting the next word in a piece of text, we're interested in predicting the next note in a piece of music.

This setup is interesting because we will be able to feed the model an **initial melody** (*seed*) and the model will be able to **generate** by itself its best guess for the following notes.   

# Dataset

First of all, we need to decide what we want to *model*; put in other words, we need to understand how we can teach our synthetic musician to *replicate* the behavior of a real musician.

To achieve this objective the most common way used is to consider a fixed **dataset**: a database of musical data that we use as a "ground truth": the objective of the model is to replicate the behavior of these songs.

The songs used were picked from an [archive of fan-made Pokémon MIDIs](https://drive.google.com/drive/u/0/folders/1C1mRfK0XVaeUo7nOHnKsZs5wHZua89Fx): the songs selected were chosen for their overall semplicity.


# Encoding
Treating data sequence is extremely (computationally) hard: it is necessary to perform semplifications in order to set up a treatable task.

The following operations were performed:

* The **highest pitched** note was extracted from each chord (i.e. a group of notes played at the same time).
* The **range** of the possible notes was reduced to 3 octaves (37 notes, from F#2 to F#5).
* The music was **transposed** to the key of C major.
* Notes **duration** was **fixed**: the pieces produced are sequence of notes with equal duration. 


Here is an example, based by Pokémon RBY - Celadon City:

<audio controls>
  <source src="https://bouncybutton.github.io/content/midi/musicgen1/midi%20inputs/Pokemon RedBlueYellow - Celadon City.ogg" type="audio/ogg">
</audio>

# Results

A long experimentation was conducted to reach the following results. In a future blog post I will describe better what process was applied to create this model.


## The first good one
The first good quality sample was the following: this piece exhibits high quality, with appropriate key changes (green) during the piece and the repetition of a certain melody, even over the bars reported (yellow). This model was discarded for its high *overfitting*, that led to always generate this melody.

![](https://bouncybutton.github.io/images/music/figure_2_1.png)
![](https://bouncybutton.github.io/images/music/figure_2_2.png)

<audio controls>
  <source src="https://bouncybutton.github.io/content/midi/musicgen1/midi%20outputs/figure2_cut.ogg" type="audio/ogg">
</audio>

## Generalizing
This piece, generated from a pentatonic scale, exhibits interesting qualities, in particular the bass line is coherent: an A major chord (yellow) is outlined by playing E-A-C# (an uncommon chord in the C major key, but quite effective nonetheless). The bass line then resolves (quite reasonably) up to a semitone, to a D (green); then, the piece gravitates toward the tonic, with a descending scale (red) ending in C. Octaves are often exploited.

![](https://bouncybutton.github.io/images/music/figure_3_1.png)
![](https://bouncybutton.github.io/images/music/figure_3_2.png)

<audio controls>
  <source src="https://bouncybutton.github.io/content/midi/musicgen1/midi%20outputs/figure3_cut.ogg" type="audio/ogg">
</audio>

This other piece is quite interesting since shows a non-diatonic arpeggio (yellow) that outlines a chord built in fourths at the end, with a simple but realistic riff at the end (green), that ends on the tonic. The non-diatonic chord arpeggio is never found in the training set, suggesting a presence of generalization in the model.

![](https://bouncybutton.github.io/images/music/figure_4_1.png)
![x](https://bouncybutton.github.io/images/music/figure_4_2.png)

<audio controls>
  <source src="https://bouncybutton.github.io/content/midi/musicgen1/midi%20outputs/figure4_cut.ogg" type="audio/ogg">
</audio>

## Completing melodies

This final piece shows what happens if we seed the model (seed until about 0:13) with a song from the dataset: the notes are preserved for two and a half measures with minimal variation (red), suggesting the capability of the model to remember and associate music. 

![](https://bouncybutton.github.io/images/music/figure_5_1.png)

<audio controls>
  <source src="https://bouncybutton.github.io/content/midi/musicgen1/midi%20outputs/figure5.ogg" type="audio/ogg">
</audio>

# Implementation tips

* The dataset used should be quite **small**: a greater dataset, if not properly encoded in a effective way, is much harder to model, since there is not always a "right note" to play in a given moment.

* Experimentation with bidirectional LSTMs slowed the training process significantly and hasn’t given any tangible qualitative advantage.

* The Adam optimizer behaved quite better than RMSProp and SGD.

*	**Random cropping** with **repetition**: the musical pieces selected were composed in such a way to be played on loop; so, it’s possible to train the network on audio fragments that use information at the end and at the beginning of the track. This is a form of *data augmentation*; it helps with a small dataset, improves generalization and reduces overfitting, despite increasing training time. 

* The input could be even more simplified by removing rare notes (such as non-diatonic notes).

* Sampling from the final distribution can be done with a softmax function with a different base. The script in the repository implements this sampling.

# Previous works

This work is based mainly on the tutorial edited by
[Sigurdur Skuli](https://towardsdatascience.com/how-to-generatemusic-using-a-lstm-neural-network-in-keras-68786834d4c5). His work aims to use a LSTM to produce a sequence of chords and notes, based on a classical music dataset. 
The main achievements reached by the author were the following:

* Exploiting the [Music21 library](https://web.mit.edu/music21/) to abstract the music generation process, starting by some MIDI files;
* Representing music as a sequence of evenly spaced group of notes (i.e. single notes or chords) in a limited [normal form](https://web.mit.edu/music21/doc/moduleReference/moduleChord.html#music21.chord.Chord.normalOrder) that doesn’t consider the notes octave.

Another work by [Magenta](https://www.google.com/doodles/celebrating-johannsebastian-bach) publicized by Google, named Coconet, uses a different approach. By restricting the domain to Bach chorales that have 4 different voices (a voice produces a single note for each instant) they managed to create a public doodle that enables the user to input a simple melody to be harmonized by the model. 


